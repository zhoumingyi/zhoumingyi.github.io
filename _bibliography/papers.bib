---
---

@inproceedings{lin2025uicompass,
  title={UICOMPASS: UI Map Guided Mobile Task Automation via Adaptive Action Generation}, 
  author={Yuanzhang Lin and Zhe Zhang and Qingao Dong and Mingyi Zhou and Jing Zhang and Xiang Gao and Hailong Sun},
  booktitle={Accepted by EMNLP'25 Main},
  year={2025},
}

@article{fan2025imu,
  title={IMU: Influence-guided Machine Unlearning},
  author={Fan, Xindi and Wu, Jing and Zhou, Mingyi and Liang, Pengwei and Phung, Dinh},
  journal={arXiv preprint arXiv:2508.01620},
  year={2025},
}

@inproceedings{lin2025mazeBreaker,
  title={MazeBreaker: Multi-Agent Reinforcement Learning for Dynamic Jailbreaking of LLM Security Defenses}, 
  author={Zhihao Lin and Wei Ma and Mingyi Zhou and Yanjie Zhao and Haoyu Wang and Yang Liu and Jun Wang and Li Li},
  booktitle={Accepted by ICSE'26},
  year={2026},
}

@inproceedings{liu2025haptest,
  title={HapTest: The Dynamic Analysis Framework for OpenHarmony},
  author={Liu*, Farong and Zhou*, Mingyi and Zhang, Yakun and Su, Ting and Sun, Bo and Klein, Jacques and Gao, Xiang and Li, Li},
  booktitle={Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
  pages={422--431},
  year={2025},
  bibtex_show={true},
  code={https://github.com/SMAT-Lab/HapTest},
  url={https://doi.org/10.1145/3696630.3728565},
  doi={10.1145/3696630.3728565},
}

@inproceedings{lin2025haprepair,
  title={HapRepair: Learn to Repair OpenHarmony Apps},
  author={Lin*, Zhihao and Zhou*, Mingyi and Ma, Wei and Chen, Chi and Yang, Yun and Wang, Jun and Hu, Chunming and Li, Li},
  booktitle={Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering},
  pages={319--330},
  year={2025},
  bibtex_show={true},
  code={https://github.com/SMAT-Lab/HapRepair},
  url={https://doi.org/10.1145/3696630.3728556},
  doi={10.1145/3696630.3728556},
}

@inproceedings{11121726,
  author={Chen, Haonan and Chen, Daihang and Yang, Yizhuo and Xu, Lingyun and Gao, Liang and Zhou, Mingyi and Hu, Chunming and Li, Li},
  booktitle={2025 IEEE/ACM 47th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)}, 
  title={ArkAnalyzer: The Static Analysis Framework for OpenHarmony}, 
  year={2025},
  volume={},
  number={},
  pages={136-147},
  keywords={Computer languages;Privacy;Codes;Operating systems;Focusing;Static analysis;Detectors;Maintenance engineering;Leak detection;Software engineering;n/a},
  doi={10.1109/ICSE-SEIP66354.2025.00018},
  bibtex_show={true},
  url={https://doi.org/10.1109/ICSE-SEIP66354.2025.00018},
  code={https://gitcode.com/openharmony-sig/arkanalyzer},
}

@article{10.1145/3708528,
  author = {Chen, Daihang and Liu, Yonghui and Zhou, Mingyi and Zhao, Yanjie and Wang, Haoyu and Wang, Shuai and Chen, Xiao and Bissyande, Tegawende F. and Klein, Jacques and Li, Li},
  title = {LLM for Mobile: An Initial Roadmap},
  year = {2024},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  issn = {1049-331X},
  url = {https://doi.org/10.1145/3708528},
  doi = {10.1145/3708528},
  abstract = {When mobile meets LLMs, mobile app users deserve to have more intelligent usage experiences. For this to happen, we argue that there is a strong need to apply LLMs for the mobile ecosystem. We therefore provide a research roadmap for guiding our fellow researchers to achieve that as a whole. In this roadmap, we sum up six directions that we believe are urgently required for research to enable native intelligence in mobile devices. In each direction, we further summarize the current research progress and the gaps that still need to be filled by our fellow researchers.},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  month = dec,
  bibtex_show={true},
  pdf={TOSEM_LLM4Mobile.pdf},
}

@inproceedings{10.1145/3691620.3694998,
  author = {Zhou, Mingyi and Gao, Xiang and Chen, Xiao and Chen, Chunyang and Grundy, John and Li, Li},
  title = {DynaMO: Protecting Mobile DL Models through Coupling Obfuscated DL Operators},
  year = {2024},
  isbn = {9798400712487},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3691620.3694998},
  doi = {10.1145/3691620.3694998},
  abstract = {Deploying deep learning (DL) models on mobile applications (Apps) has become ever-more popular. However, existing studies show attackers can easily reverse-engineer mobile DL models in Apps to steal intellectual property or generate effective attacks. A recent approach, Model Obfuscation, has been proposed to defend against such reverse engineering by obfuscating DL model representations, such as weights and computational graphs, without affecting model performance. These existing model obfuscation methods use static methods to obfuscate the model representation, or they use half-dynamic methods but require users to restore the model information through additional input arguments. However, these static methods or half-dynamic methods cannot provide enough protection for on-device DL models. Attackers can use dynamic analysis to mine the sensitive information in the inference codes as the correct model information and intermediate results must be recovered at runtime for static and half-dynamic obfuscation methods. We assess the vulnerability of the existing obfuscation strategies using an instrumentation method and tool, DLModelExplorer, that dynamically extracts correct sensitive model information (i.e., weights, computational graph) at runtime. Experiments show it achieves very high attack performance (e.g., 98.76\% of weights extraction rate and 99.89\% of obfuscating operator classification rate). To defend against such attacks based on dynamic instrumentation, we propose DynaMO, a Dynamic Model Obfuscation strategy similar to Homomorphic Encryption. The obfuscation and recovery process can be done through simple linear transformation for the weights of randomly coupled eligible operators, which is a fully dynamic obfuscation strategy. Experiments show that our proposed strategy can dramatically improve model security compared with the existing obfuscation strategies, with only negligible overheads for on-device models. Our prototype tool is publicly available at https://github.com/zhoumingyi/DynaMO.},
  booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
  pages = {204–215},
  numpages = {12},
  keywords = {SE for AI, AI safety, on-device AI},
  location = {Sacramento, CA, USA},
  series = {ASE '24},
  bibtex_show={true},
  preview={ASE2024_DynaMO.png},
  pdf={ASE2024_DynaMO.pdf},
  code={https://github.com/zhoumingyi/DynaMO},
  selected={true},
}

@inproceedings{10.1145/3650212.3652119,
  author = {Zhou, Mingyi and Gao, Xiang and Liu, Pei and Grundy, John and Chen, Chunyang and Chen, Xiao and Li, Li},
  title = {Model-less Is the Best Model: Generating Pure Code Implementations to Replace On-Device DL Models},
  year = {2024},
  isbn = {9798400706127},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3650212.3652119},
  doi = {10.1145/3650212.3652119},
  abstract = {Recent studies show that on-device deployed deep learning (DL) models, such as those of Tensor Flow Lite (TFLite), can be easily extracted from real-world applications and devices by attackers to generate many kinds of adversarial and other attacks. Although securing deployed on-device DL models has gained increasing attention, no existing methods can fully prevent these attacks. Traditional software protection techniques have been widely explored. If on-device models can be implemented using pure code, such as C++, it will open the possibility of reusing existing robust software protection techniques. However, due to the complexity of DL models, there is no automatic method that can translate DL models to pure code. To fill this gap, we propose a novel method, CustomDLCoder, to automatically extract on-device DL model information and synthesize a customized executable program for a wide range of DL models. CustomDLCoder first parses the DL model, extracts its backend computing codes, configures the extracted codes, and then generates a customized program to implement and deploy the DL model without explicit model representation. The synthesized program hides model information for DL deployment environments since it does not need to retain explicit model representation, preventing many attacks on the DL model. In addition, it improves ML performance because the customized code removes model parsing and preprocessing steps and only retains the data computing process. Our experimental results show that CustomDLCoder improves model security by disabling on-device model sniffing. Compared with the original on-device platform (i.e., TFLite), our method can accelerate model inference by 21.0\% and 24.3\% on x86-64 and ARM64 platforms, respectively. Most importantly, it can significantly reduce memory consumption by 68.8\% and 36.0\% on x86-64 and ARM64 platforms, respectively.},
  booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
  pages = {174–185},
  numpages = {12},
  keywords = {AI safety, SE for AI, software optimization for AI deployment},
  location = {Vienna, Austria},
  series = {ISSTA 2024},
  bibtex_show={true},
  preview={ISSTA2024_DLCustomCoder.png},
  pdf={ISSTA2024_DLCustomCoder.pdf},
  code={https://github.com/zhoumingyi/CustomDLCoder},
  selected={true},
}

@article{Wu_Hayat_Zhou_Harandi_2024,
  title={Concealing Sensitive Samples against Gradient Leakage in Federated Learning},
  volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/30171},
  DOI={10.1609/aaai.v38i19.30171},
  abstractNote={Federated Learning (FL) is a distributed learning paradigm that enhances users’ privacy by eliminating the need for clients to share raw, private data with the server.
  Despite the success, recent studies expose the vulnerability of FL to model inversion attacks, where adversaries reconstruct users’ private data via eavesdropping on the shared gradient information. We hypothesize that a key factor in the success of such attacks is the low entanglement among gradients per data within the batch during stochastic optimization. This creates a vulnerability that an adversary can exploit to reconstruct the sensitive data. Building upon this insight, we present a simple, yet effective defense strategy that obfuscates the gradients of the sensitive data with concealed samples. To achieve this, we propose synthesizing concealed samples to mimic the sensitive data at the gradient level while ensuring their visual dissimilarity from the actual sensitive data. Compared to the previous art, our empirical evaluations suggest that the proposed technique provides the strongest protection while simultaneously maintaining the FL performance. Code is located at https://github.com/JingWu321/DCS-2.},
  number={19},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  author={Wu, Jing and Hayat, Munawar and Zhou, Mingyi and Harandi, Mehrtash},
  year={2024},
  month={Mar.},
  pages={21717-21725},
  bibtex_show={true},
  preview={AAAI2024_DCS2.png},
  pdf={AAAI2024_DCS2.pdf},
  code={https://github.com/JingWu321/DCS-2},
  selected={true},
}

@inproceedings{10.1145/3597503.3639144,
  author = {Zhou, Mingyi and Gao, Xiang and Wu, Jing and Liu, Kui and Sun, Hailong and Li, Li},
  title = {Investigating White-Box Attacks for On-Device Models},
  year = {2024},
  isbn = {9798400702174},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3597503.3639144},
  doi = {10.1145/3597503.3639144},
  abstract = {Numerous mobile apps have leveraged deep learning capabilities. However, on-device models are vulnerable to attacks as they can be easily extracted from their corresponding mobile apps. Although the structure and parameters information of these models can be accessed, existing on-device attacking approaches only generate black-box attacks (i.e., indirect white-box attacks), which are less effective and efficient than white-box strategies. This is because mobile deep learning (DL) frameworks like TensorFlow Lite (TFLite) do not support gradient computing (referred to as non-debuggable models), which is necessary for white-box attacking algorithms. Thus, we argue that existing findings may underestimate the harm-fulness of on-device attacks. To validate this, we systematically analyze the difficulties of transforming the on-device model to its debuggable version and propose a Reverse Engineering framework for On-device Models (REOM), which automatically reverses the compiled on-device TFLite model to its debuggable version, enabling attackers to launch white-box attacks. Our empirical results show that our approach is effective in achieving automated transformation (i.e., 92.6\%) among 244 TFLite models. Compared with previous attacks using surrogate models, REOM enables attackers to achieve higher attack success rates (10.23\%→89.03\%) with a hundred times smaller attack perturbations (1.0→0.01). Our findings emphasize the need for developers to carefully consider their model deployment strategies, and use white-box methods to evaluate the vulnerability of on-device models. Our artifacts 1 are available.},
  booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
  articleno = {152},
  numpages = {12},
  location = {Lisbon, Portugal},
  series = {ICSE '24},
  bibtex_show={true},
  preview={ICSE2024_REOM.png},
  pdf={ICSE2024_REOM.pdf},
  code={https://github.com/zhoumingyi/REOM},
  selected={true},
}

@inproceedings{10.1145/3597926.3598113,
  author = {Zhou, Mingyi and Gao, Xiang and Wu, Jing and Grundy, John and Chen, Xiao and Chen, Chunyang and Li, Li},
  title = {ModelObfuscator: Obfuscating Model Information to Protect Deployed ML-Based Systems},
  year = {2023},
  isbn = {9798400702211},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3597926.3598113},
  doi = {10.1145/3597926.3598113},
  abstract = {More and more edge devices and mobile apps are leveraging deep learning (DL) capabilities. Deploying such models on devices – referred to as on-device models – rather than as remote cloud-hosted services, has gained popularity because it avoids transmitting user’s data off of the device and achieves high response time. However, on-device models can be easily attacked, as they can be accessed by unpacking corresponding apps and the model is fully exposed to attackers. Recent studies show that attackers can easily generate white-box-like attacks for an on-device model or even inverse its training data. To protect on-device models from white-box attacks, we propose a novel technique called model obfuscation. Specifically, model obfuscation hides and obfuscates the key information – structure, parameters and attributes – of models by renaming, parameter encapsulation, neural structure obfuscation, shortcut injection, and extra layer injection. We have developed a prototype tool ModelObfuscator to automatically obfuscate on-device TFLite models. Our experiments show that this proposed approach can dramatically improve model security by significantly increasing the difficulty of parsing models’ inner information, without increasing the latency of DL models. Our proposed on-device model obfuscation has the potential to be a fundamental technique for on-device model deployment. Our prototype tool is publicly available at https://github.com/zhoumingyi/ModelObfuscator.},
  booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
  pages = {1005–1017},
  numpages = {13},
  keywords = {AI safety, SE for AI, model deployment, model obfuscation},
  location = {Seattle, WA, USA},
  series = {ISSTA 2023},
  bibtex_show={true},
  preview={ISSTA2023_ModelObfuscator.png},
  pdf={ISSTA2023_ModelObfuscator.pdf},
  code={https://github.com/zhoumingyi/ModelObfuscator},
  selected={true},
}

@INPROCEEDINGS{zhou2020dast,
  author={Zhou*, Mingyi and Wu*, Jing and Liu, Yipeng and Liu, Shuaicheng and Zhu, Ce},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={[Oral, Top 4.5%] DaST: Data-Free Substitute Training for Adversarial Attacks}, 
  year={2020},
  pages={231-240},
  doi={10.1109/CVPR42600.2020.00031},
  bibtex_show={true},
  pdf={CVPR2020_DaST.pdf},
  preview={CVPR2020_DaST.png},
  code={https://github.com/zhoumingyi/DaST},
  selected={true}
}

@article{zhou2019tensor,
  title={Tensor rank learning in CP decomposition via convolutional neural network},
  author={Zhou, Mingyi and Liu, Yipeng and Long, Zhen and Chen, Longxi and Zhu, Ce},
  journal={Signal Processing: Image Communication},
  volume={73},
  pages={12--21},
  year={2019},
  publisher={Elsevier},
  selected={true},
}

